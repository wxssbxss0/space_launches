# COE 332 Final Project: Space Launches API üöÄ & Kubernetes Pipeline

## Overview
The **Space Launches Data API** is a containerized microservice suite that:

1. **Fetches & normalizes** the Kaggle ‚ÄúOne Small Step for Data‚Äù Global Space Launches CSV, handling header quirks and imputing missing rocket counts.

2. **Caches** every launch record in Redis (db 0) for low-latency access.

3. Exposes a **Flask REST API** to load (/data), query (GET /data), and delete (DELETE /data) launch records.

4. Provides a suite of analysis endpoints‚Äî/analyze/timeline, /analyze/sector, /analyze/geography, and /analyze/top-private‚Äîwhich **enqueue jobs** for background processing.

5. Offers a **Jobs API** (/jobs/<job_id>) so clients can poll job status and a **Results API** (/results/<job_id>) to download the generated PNG plots.

6. Runs a **Worker** service that pulls tasks off a Redis list (db 1), updates job metadata (db 2), runs the appropriate plotting routine (timeline crossover, sector bar, geographic heatmap, top-private bar), and stores the raw PNG bytes in Redis (db 3).
---

## Introduction  
We provide a containerized Flask+Redis API for ingesting, normalizing, and analyzing a global space-launches dataset.  Users can submit background jobs to generate **timeline**, **sector**, **geography** and **top-private provider** plots‚Äî then download the resulting PNGs.

---

## Data Preparation  
1. **Kaggle CSV**: ‚ÄúOne Small Step for Data‚Äù Global Space Launches (https://www.kaggle.com/datasets/davidroberts13/one-small-step-for-data)  
2. **Kaggle API Token**:  
   - Copy your `kaggle.json` token into `~/.kaggle/kaggle.json` (inside your VM or Docker host).  
   - Lock down permissions:  
     ```bash
     chmod 600 ~/.kaggle/kaggle.json
     ```  
3. **Columns of Interest**:  
   - `Year`, `Private or State Run` (P/S), `Company Name`, `Country of Launch`, `# Rocket`  

---

## Architecture Diagram  
![Architecture Diagram](diagram.png)

---

## Key Technologies  
- **Docker & Docker Compose** for local container orchestration  
- **Flask** for REST API endpoints  
- **Redis** (db 0‚Ä¶3) for record storage, job queue, metadata, and result caching  
- **Pandas & Matplotlib** for data cleaning and static plotting  
- **(Kubernetes)** manifests (in `kubernetes/`) for future production rollout  

---

## Project Overview

This project performs the following operations:

1. **Space Launch Data Handling:**  
   - Downloads (via Kaggle API) or reads a local CSV of global space launches.  
   - Normalizes headers (strips whitespace), imputes missing `# Rocket` values, and resolves ‚ÄúSea Launch‚Äù entries.  
   - Caches every launch record in Redis (db 0) for fast retrieval.  
   - Exposes endpoints to load (`POST /data`), list (`GET /data`), and delete (`DELETE /data`) launch records.

2. **Jobs API:**  
   - Provides analysis endpoints that enqueue background jobs for plotting:  
     - `/analyze/timeline` (crossover analysis)  
     - `/analyze/sector` (private vs. state bar chart)  
     - `/analyze/geography` (country heatmap)  
     - `/analyze/top-private` (top-10 private providers)  
   - Each job is assigned a UUID and stored in Redis (db 1 as a list, db 2 for metadata).  
   - Clients poll `/jobs/<job_id>` to check status and `/results/<job_id>` to download the resulting PNG.

3. **Worker Integration:**  
   A dedicated worker process continuously:  
   - BLPOPs a job ID from Redis db 1, marks it ‚Äúin progress‚Äù in db 2.  
   - Loads the full launch cache from Redis db 0.  
   - Runs the appropriate plotting function (`plot_private_crossover`, `plot_sector`, `plot_geography`, or `plot_top_private`).  
   - Stores the raw PNG bytes under `results:<job_id>` in Redis db 3 and marks the job ‚Äúcomplete‚Äù in db 2.


| Route                      | Method | Description                                                                              |
| -------------------------- | :----: | ---------------------------------------------------------------------------------------- |
| **`/help`**                |   GET  | List all endpoints and usage.                                                            |
| **`/data`**                |   GET  | Get all records (seeds from Kaggle CSV if empty).                                        |
|                            |  POST  | Upload CSV (`file`) or JSON array/object to seed/append records.                         |
|                            | DELETE | Delete all records.                                                                      |
| **`/analyze/timeline`**    |  POST  | Enqueue crossover analysis: marks the first year private launches exceed state launches. |
| **`/analyze/sector`**      |  POST  | Enqueue sector comparison: bar chart of private vs. state launches.                      |
| **`/analyze/geography`**   |  POST  | Enqueue geographic analysis: heatmap of launches by country.                             |
| **`/analyze/top-private`** |  POST  | Enqueue top-private job: top 10 private launch providers (1995‚Äì2020).                    |
| **`/jobs/<job_id>`**       |   GET  | Query job status & metadata (`queued`/`complete`, type, `result_ready` flag).            |
| **`/results/<job_id>`**    |   GET  | Download the PNG bytes for a completed job.                                              |


## Main Files

- **api.py**  
  The Flask application that defines all HTTP endpoints:  
  - **`/help`** ‚Äî Lists every route and usage.  
  - **`/data`** ‚Äî `POST` to seed or append launches, `GET` to list all records, `DELETE` to clear the dataset.  
  - **`/analyze/timeline`**, **`/analyze/sector`**, **`/analyze/geography`**, **`/analyze/top-private`** ‚Äî Enqueue background analysis jobs.  
  - **`/jobs/<job_id>`** ‚Äî Query job status and metadata.  
  - **`/results/<job_id>`** ‚Äî Download the PNG result for a completed job.

- **jobs.py**  
  Defines the job‚Äêmanagement functions used by both `api.py` and `worker.py`:  
  1. `create_job(job_type)` ‚Äì generates a UUID, stores metadata in Redis db 2, and pushes the job ID onto the queue (db 1).  
  2. `get_job(job_id)` ‚Äì retrieves status/type/parameters from Redis db 2.  
  3. `job_result_ready(job_id)` ‚Äì checks for a result entry in Redis db 3.  

- **launches_reader.py**  
  Handles CSV ingestion and Kaggle API fetch:  
  1. Reads a local file‚Äêlike or downloads+unzips the Kaggle dataset.  
  2. Strips whitespace from headers, converts ‚Äú# Rocket‚Äù to numeric and imputes missing values.  
  3. Returns a list of JSON‚Äêserializable records for storage in Redis.

- **worker.py**  
  The background‚Äêworker process that:  
  1. Uses `BLPOP` on Redis db 1 to retrieve job IDs.  
  2. Marks jobs ‚Äúin progress‚Äù in Redis db 2.  
  3. Loads cached launch records from Redis db 0.  
  4. Dispatches to one of four plotting functions (`plot_private_crossover`, `plot_sector`, `plot_geography`, `plot_top_private`).  
  5. Stores the PNG bytes under `results:<job_id>` in Redis db 3 and marks jobs ‚Äúcomplete‚Äù in db 2.

- **Dockerfile**  
  Builds a single image for both the API and worker:  
  - Base: `python:3.12-slim`.  
  - Installs dependencies from `requirements.txt`.  
  - Copies `src/` code in and sets `WORKDIR` to `/app/src`.  
  - Exposes port 5000 and defaults to running `python api.py`.

- **docker-compose.yml**  
  Orchestrates three services:  
  - **`api`** (builds the Dockerfile, runs `python api.py`, exposes port 5000)  
  - **`worker`** (same image, runs `python worker.py`)  
  - **`redis`** (official Redis 7 image, with persistent volume)

- **pytest.ini**  
  Configures pytest to discover tests in the `test/` directory and apply any custom markers.

- **test/**  
  Contains unit & integration tests:  
  - `test_api.py` verifies all Flask endpoints (`/data`, `/analyze/*`, `/jobs/*`, `/results/*`).  
  - `test_worker.py` verifies each plotting function returns valid PNG bytes.

- **kubernetes/**  
  *(Future work)* Helm charts and Kubernetes manifests for deploying API, worker, and Redis in a cluster.


## Unit Tests

We now maintain three test suites under `test/` that validate every layer of our application:

- **`test_api.py`**  
  - Verifies all Flask endpoints (`/help`, `/data`, `/analyze/timeline`, `/analyze/sector`, `/analyze/geography`, `/analyze/top-private`, `/jobs/<job_id>`, `/results/<job_id>`).  
  - Spins up the app in `TESTING` mode and exercises CRUD and job‚Äêenqueueing workflows against a real Redis instance.

- **`test_jobs.py`**  
  - Exercises the core job‚Äêmanagement functions in `jobs.py`:  
    - `create_job()`, `get_job()`, status transitions, and `job_result_ready()`.  
  - Uses a mocked or local Redis server to confirm metadata is stored and the queue is pushed correctly.

- **`test_worker.py`**  
  - Validates each plotting routine‚Äî`plot_private_crossover()`, `plot_sector()`, `plot_geography()`, and `plot_top_private()`‚Äîreturns valid PNG bytes (checks the PNG magic header).  
  - Runs against small in‚Äêmemory DataFrames and record lists.

---

### Running the tests

1. **Start Redis** (in a separate terminal or background):
   ```bash
   cd test
   redis-server --daemonize yes
2. **Configure the Environment**
   ```bash
   export REDIS_HOST=127.0.0.1
3. **Run Pytest**
   ```bash
   python3 -m pytest

---

